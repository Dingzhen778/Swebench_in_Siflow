#!/usr/bin/env python3
"""
è¿è¡Œgold patchè¯„æµ‹ - ä¿®å¤ç‰ˆæœ¬

ä¸¥æ ¼éµå¾ªSWE-benchçš„è¯„ä¼°é€»è¾‘:
1. åº”ç”¨ gold/model patch åˆ°æºä»£ç 
2. é‡æ–°å®‰è£…ä»“åº“ (python -m pip install -e .)
3. é‡ç½®æµ‹è¯•æ–‡ä»¶åˆ° base_commit
4. åº”ç”¨ test patch
5. è¿è¡Œæµ‹è¯•
6. è§£æç»“æœ
"""

import sys
import json
import time
from pathlib import Path
from datasets import load_dataset

from siflow.types import TaskVolume, TaskEnv, TaskUserSelectedInstance
from siflow_utils import create_siflow_client, get_image_registry_url
from siflow_config import RESOURCE_POOL, INSTANCE_TYPE
from swebench.harness.constants import MAP_REPO_VERSION_TO_SPECS, FAIL_TO_PASS, PASS_TO_PASS, START_TEST_OUTPUT, END_TEST_OUTPUT
from swebench.harness.test_spec.python import get_test_directives, get_modified_files
from swebench.harness.test_spec.test_spec import TestSpec, make_test_spec
from swebench.harness.grading import get_eval_tests_report, get_resolution_status
from swebench.harness.log_parsers import MAP_REPO_TO_PARSER
from fix_build_issues import should_apply_fix


def get_image_version_for_instance(instance_id: str) -> str:
    """
    è·å–instanceåº”è¯¥ä½¿ç”¨çš„é•œåƒç‰ˆæœ¬

    å¦‚æœinstanceéœ€è¦åº”ç”¨ä¿®å¤è¡¥ä¸ï¼Œä½¿ç”¨2.1.0ç‰ˆæœ¬ï¼ˆå·²ä¿®å¤ï¼‰
    å¦åˆ™ä½¿ç”¨2.0.0ç‰ˆæœ¬ï¼ˆåŸå§‹ï¼‰

    æ³¨æ„ï¼šæ­£åœ¨è¿ç§»åˆ°ç»Ÿä¸€2.0.0ç‰ˆæœ¬ï¼Œè¿ç§»å®Œæˆå‰ä¿æŒåŒç‰ˆæœ¬
    """
    if should_apply_fix(instance_id):
        return "2.1.0"  # ä¿®å¤åçš„é•œåƒï¼ˆä¸´æ—¶ï¼‰
    return "2.0.0"  # åŸå§‹é•œåƒ


def generate_eval_script_fixed(instance, specs, patch_file_path, test_patch_file_path):
    """
    ç”Ÿæˆè¯„ä¼°è„šæœ¬ - ä¸¥æ ¼éµå¾ªSWE-benché€»è¾‘

    Args:
        instance: dataset instance
        specs: é…ç½®è§„èŒƒ
        patch_file_path: patchæ–‡ä»¶è·¯å¾„ (.diffæˆ–.agentless_raw)
        test_patch_file_path: test patchæ–‡ä»¶è·¯å¾„

    å…³é”®é¡ºåº:
    0. (å¦‚æœæ˜¯.agentless_raw) åº”ç”¨SEARCH/REPLACEå¹¶ç”Ÿæˆdiff
    1. åº”ç”¨patchåˆ°æºä»£ç 
    2. é‡æ–°å®‰è£…ä»“åº“
    3. é‡ç½®æµ‹è¯•æ–‡ä»¶åˆ° base_commit
    4. åº”ç”¨ test patch
    5. è¿è¡Œæµ‹è¯•
    """
    instance_id = instance['instance_id']
    repo = instance['repo']
    base_commit = instance['base_commit']
    test_patch = instance['test_patch']

    env_name = "testbed"
    repo_directory = f"/{env_name}"

    # è·å–æµ‹è¯•æŒ‡ä»¤
    test_directives = get_test_directives(instance)
    test_command = specs.get('test_cmd', 'pytest')
    test_targets = ' '.join(test_directives) if test_directives else ''

    # è·å–test patchä¿®æ”¹çš„æµ‹è¯•æ–‡ä»¶
    test_files = get_modified_files(test_patch)

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    test_output_file = f"/volume/ai-infra/rhjiang/SWE-bench-cc/siflow/3-layer-test/eval_outputs/{instance_id}_test_output.txt"

    # ç”Ÿæˆè„šæœ¬ - ä¸¥æ ¼æŒ‰ç…§SWE-benchçš„é¡ºåº
    # ä½¿ç”¨å•å¼•å·åŒ…è£¹æ•´ä¸ªbashå‘½ä»¤ï¼Œé¿å…åŒå¼•å·åµŒå¥—é—®é¢˜
    script_lines = [
        "bash -c '",
        'set -uxo pipefail &&',
        '',
        'echo "========================================" &&',
        'echo "Step 0: Setup" &&',
        'echo "========================================" &&',
        'source /opt/miniconda3/bin/activate &&',
        f'conda activate {env_name} &&',
        f'cd {repo_directory} &&',
        'mkdir -p /volume/ai-infra/rhjiang/SWE-bench-cc/siflow/3-layer-test/eval_outputs &&',
        '',
    ]

    # å¦‚æœæ˜¯agentlessæ ¼å¼ï¼Œå…ˆè½¬æ¢
    if '.agentless_raw' in str(patch_file_path):
        converted_patch = str(patch_file_path).replace('.agentless_raw', '_converted.diff')
        apply_script_path = '/volume/ai-infra/rhjiang/SWE-bench-cc/siflow/3-layer-test/apply_agentless.py'
        parser_script_path = '/volume/ai-infra/rhjiang/SWE-bench-cc/siflow/3-layer-test/agentless_parser.py'

        script_lines.extend([
            'echo "" &&',
            'echo "========================================" &&',
            'echo "Step 0.5: Convert Agentless SEARCH/REPLACE to diff" &&',
            'echo "========================================" &&',
            f'cp {parser_script_path} ./agentless_parser.py &&',
            f'python3 {apply_script_path} {patch_file_path} &&',
            f'git diff > {converted_patch} &&',
            'git checkout . &&',
            f'echo "âœ“ Generated unified diff: {converted_patch}" &&',
            '',
        ])
        # ä½¿ç”¨è½¬æ¢åçš„diff
        actual_patch = converted_patch
    else:
        actual_patch = patch_file_path

    script_lines.extend([
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 1: Apply Model Patch" &&',
        'echo "========================================" &&',
        f'if [ -f {actual_patch} ]; then',
        f'    git apply --verbose {actual_patch} || patch --batch --fuzz=5 -p1 -i {actual_patch} || exit 1',
        '    echo "Model patch applied successfully"',
        'else',
        f'    echo "ERROR: Patch file not found: {actual_patch}"',
        '    exit 1',
        'fi &&',
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 2: Record git info" &&',
        'echo "========================================" &&',
        f'git config --global --add safe.directory {repo_directory} &&',
        f'cd {repo_directory} &&',
        'git status &&',
        'git show &&',
        f'git -c core.fileMode=false diff {base_commit} &&',
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 3: Reinstall repository" &&',
        'echo "========================================" &&',
        'source /opt/miniconda3/bin/activate &&',
        f'conda activate {env_name} &&',
    ])

    # æ¸…ç† Python ç¼“å­˜ï¼ˆé˜²æ­¢ .pyc æ–‡ä»¶è¿‡æ—¶ï¼‰
    script_lines.extend([
        'echo "Cleaning Python cache..." &&',
        f'find {repo_directory} -type d -name __pycache__ -exec rm -rf {{}} + 2>/dev/null || true &&',
        f'find {repo_directory} -type f -name "*.pyc" -delete 2>/dev/null || true &&',
    ])

    # æ·»åŠ å®‰è£…å‘½ä»¤
    install_cmd = specs.get('install', 'python -m pip install -e .')
    script_lines.extend([
        f'{install_cmd} &&',
        'echo "Cleaning Python cache again after install..." &&',
        f'find {repo_directory} -type d -name __pycache__ -exec rm -rf {{}} + 2>/dev/null || true &&',
        f'find {repo_directory} -type f -name "*.pyc" -delete 2>/dev/null || true &&',
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 4: Reset test files to base commit" &&',
        'echo "========================================" &&',
    ])

    # é‡ç½®æµ‹è¯•æ–‡ä»¶
    if test_files:
        script_lines.append(f'git checkout {base_commit} {" ".join(test_files)} &&')
        script_lines.append(f'echo "Test files reset: {len(test_files)} files" &&')
    else:
        script_lines.append('echo "No test file modifications detected" &&')

    script_lines.extend([
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 5: Apply Test Patch" &&',
        'echo "========================================" &&',
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„apply test patch (SWE-benchæ ‡å‡†åšæ³•)
        f'if [ -f {test_patch_file_path} ]; then',
        f'    git apply -v {test_patch_file_path} || patch --batch --fuzz=5 -p1 -i {test_patch_file_path} || exit 1',
        '    echo "Test patch applied successfully"',
        'else',
        f'    echo "ERROR: Test patch file not found: {test_patch_file_path}"',
        '    exit 1',
        'fi &&',
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 6: Run tests" &&',
        'echo "========================================" &&',
    ])

    # æ ‡è®°æµ‹è¯•è¾“å‡ºå¼€å§‹ (ä¸SWE-benchä¸€è‡´)
    # å…ˆè¾“å‡ºSTARTæ ‡è®°åˆ°æ–‡ä»¶ï¼Œç„¶åè¿è¡Œæµ‹è¯•è¿½åŠ åˆ°åŒä¸€æ–‡ä»¶ï¼Œæœ€åè¿½åŠ ENDæ ‡è®°
    script_lines.extend([
        f'echo "{START_TEST_OUTPUT}" > {test_output_file}.tmp &&',
        '(',
        f'    export PYTHONPATH={repo_directory}:${{PYTHONPATH:-}} &&',
        f'    echo "PYTHONPATH=$PYTHONPATH" &&',
        f'    {test_command} {test_targets}',
        f') >> {test_output_file}.tmp 2>&1 ; TEST_EXIT_CODE=$? &&',
        f'echo "{END_TEST_OUTPUT}" >> {test_output_file}.tmp &&',
        f'echo "SWEBENCH_TEST_EXIT_CODE=$TEST_EXIT_CODE" >> {test_output_file}.tmp &&',
        f'cat {test_output_file}.tmp | tee {test_output_file} &&',
        f'rm -f {test_output_file}.tmp &&',
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Step 7: Restore test files" &&',
        'echo "========================================" &&',
    ])

    # æ¢å¤æµ‹è¯•æ–‡ä»¶ (ä¸SWE-benchä¸€è‡´)
    if test_files:
        script_lines.append(f'git checkout {base_commit} {" ".join(test_files)} &&')

    script_lines.extend([
        '',
        'echo "" &&',
        'echo "========================================" &&',
        'echo "Testing completed" &&',
        'echo "========================================" &&',
        'echo "Exit code: $TEST_EXIT_CODE" &&',
        'exit $TEST_EXIT_CODE',
        "'"
    ])

    return '\n'.join(script_lines)


def run_gold_eval_for_instance(instance_id, image_version=None, timeout=1800, wait=True, patch_type="gold", task_name_suffix=""):
    """
    ä¸ºå•ä¸ªinstanceè¿è¡Œpatchè¯„æµ‹ - ä½¿ç”¨ä¿®å¤åçš„è¯„ä¼°é€»è¾‘

    Args:
        instance_id: å®ä¾‹ID
        image_version: é•œåƒç‰ˆæœ¬ï¼ˆNoneæ—¶è‡ªåŠ¨é€‰æ‹©ï¼šæœ‰ä¿®å¤ç”¨2.1.0ï¼Œå¦åˆ™ç”¨2.0.0ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        wait: æ˜¯å¦ç­‰å¾…ä»»åŠ¡å®Œæˆ
        patch_type: patchç±»å‹ ("gold" æˆ– "model")
    """
    patch_type_name = "Gold Patch" if patch_type == "gold" else "Model Patch"
    print(f"\n{'='*70}")
    print(f"è¿è¡Œ {patch_type_name} è¯„æµ‹ (ä¿®å¤ç‰ˆæœ¬): {instance_id}")
    print(f"{'='*70}\n")

    # 1. ä» Dataset è·å–å®ä¾‹ä¿¡æ¯
    print("ğŸ“¥ æ­£åœ¨ä» Dataset åŠ è½½å®ä¾‹ä¿¡æ¯...")
    ds = load_dataset("princeton-nlp/SWE-bench_Verified", split="test")
    instance = [x for x in ds if x['instance_id'] == instance_id][0]

    repo = instance['repo']
    version = instance['version']

    print(f"  âœ“ Repo: {repo}")
    print(f"  âœ“ Version: {version}")

    # 2. è·å–specs
    if repo not in MAP_REPO_VERSION_TO_SPECS or version not in MAP_REPO_VERSION_TO_SPECS[repo]:
        print(f"  âŒ æœªæ‰¾åˆ°é…ç½®")
        return {"success": False, "error": "Config not found"}

    specs = MAP_REPO_VERSION_TO_SPECS[repo][version]

    # 3. è‡ªåŠ¨é€‰æ‹©é•œåƒç‰ˆæœ¬ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if image_version is None:
        image_version = get_image_version_for_instance(instance_id)
        if image_version == "2.1.0":
            print(f"  â„¹ï¸  ä½¿ç”¨ä¿®å¤åçš„é•œåƒç‰ˆæœ¬: 2.1.0")

    # 4. è¯»å–patch (ä»datasetï¼ŒSWE-benchæ ‡å‡†åšæ³•)
    print(f"\nğŸ“„ è¯»å– {patch_type} patch...")
    if patch_type == "gold":
        # Gold patchä»datasetçš„'patch'å­—æ®µè¯»å–
        gold_patch = instance['patch']
    else:
        # Model patchä»æ–‡ä»¶è¯»å–ï¼ˆå·²ç»åœ¨modelç›®å½•ï¼‰
        model_patch_dir = Path("/volume/ai-infra/rhjiang/SWE-bench-cc/predictions/model")
        model_patch_file = model_patch_dir / f"{instance_id}.diff"
        agentless_file = model_patch_dir / f"{instance_id}.agentless_raw"

        # ä¼˜å…ˆæ£€æŸ¥agentlessæ ¼å¼
        if agentless_file.exists():
            print(f"  âœ“ æ£€æµ‹åˆ° Agentless æ ¼å¼patch")
            # Agentlessæ–‡ä»¶å·²åœ¨æ­£ç¡®ä½ç½®ï¼Œç›´æ¥ä½¿ç”¨
            patch_file_path = str(agentless_file)
            gold_patch = None  # ä¸éœ€è¦è¯»å–å†…å®¹ï¼Œç›´æ¥ç”¨æ–‡ä»¶è·¯å¾„
        elif model_patch_file.exists():
            patch_file_path = str(model_patch_file)
            gold_patch = None
        else:
            print(f"  âŒ æ‰¾ä¸åˆ° model patch (.diff æˆ– .agentless_raw)")
            return {"success": False, "error": "Model patch not found"}


    # 5. å°†patchå†™å…¥volumeæŒ‚è½½çš„ç›®å½•ï¼ˆä»…gold patchéœ€è¦å†™å…¥ï¼‰
    if patch_type == "gold":
        print(f"  âœ“ Patch å¤§å°: {len(gold_patch)} å­—èŠ‚")

        # Gold patchéœ€è¦ä»datasetå†™å…¥æ–‡ä»¶
        patch_dir = Path("/volume/ai-infra/rhjiang/SWE-bench-cc/predictions/gold")
        patch_dir.mkdir(parents=True, exist_ok=True)

        patch_file = patch_dir / f"{instance_id}.diff"
        patch_file.write_text(gold_patch)
        patch_file_path = str(patch_file)
    else:
        # Model patchå·²åœ¨æ–‡ä»¶ç³»ç»Ÿï¼Œpatch_file_pathå·²è®¾ç½®
        print(f"  âœ“ Patchæ–‡ä»¶: {patch_file_path}")

    # å†™å…¥test patchæ–‡ä»¶åˆ°test_patchesç›®å½•
    test_patch = instance['test_patch']
    test_patch_dir = Path("/volume/ai-infra/rhjiang/SWE-bench-cc/predictions/test_patches")
    test_patch_dir.mkdir(parents=True, exist_ok=True)
    test_patch_file = test_patch_dir / f"{instance_id}.diff"
    test_patch_file.write_text(test_patch)
    test_patch_file_path = str(test_patch_file)
    print(f"  âœ“ Test patchå·²å†™å…¥: {test_patch_file_path}")

    # 6. åˆå§‹åŒ–å®¢æˆ·ç«¯
    print(f"\nğŸ“Œ åˆå§‹åŒ– SiFlow å®¢æˆ·ç«¯...")
    client = create_siflow_client()

    # 7. è·å– instance é•œåƒ
    from siflow_utils import sanitize_image_name
    instance_image_name = f"swebench-instance-{instance_id}"
    instance_image_name = sanitize_image_name(instance_image_name)

    print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢ instance é•œåƒ: {instance_image_name}:{image_version}")
    instance_image_url = get_image_registry_url(client, instance_image_name, image_version)
    if not instance_image_url:
        print(f"  âŒ æ‰¾ä¸åˆ° instance é•œåƒ")
        return {"success": False, "error": "Instance image not found"}

    print(f"  âœ“ Instance é•œåƒ: {instance_image_url}")

    # 8. ç”Ÿæˆè¯„ä¼°è„šæœ¬ (ä½¿ç”¨ä¿®å¤åçš„ç‰ˆæœ¬ï¼Œä¼ é€’patchæ–‡ä»¶è·¯å¾„)
    print(f"\nğŸ“ ç”Ÿæˆè¯„ä¼°è„šæœ¬ (ä¿®å¤ç‰ˆæœ¬)...")
    eval_script = generate_eval_script_fixed(instance, specs, patch_file_path, test_patch_file_path)

    script_lines = eval_script.split('\n')
    print(f"  âœ“ è„šæœ¬ç”Ÿæˆå®Œæˆ ({len(script_lines)} è¡Œ)")
    print(f"\n  å…³é”®æ­¥éª¤:")
    print(f"    1. åº”ç”¨ Gold Patch åˆ°æºä»£ç ")
    print(f"    2. é‡æ–°å®‰è£…ä»“åº“")
    print(f"    3. é‡ç½®æµ‹è¯•æ–‡ä»¶")
    print(f"    4. åº”ç”¨ Test Patch")
    print(f"    5. è¿è¡Œæµ‹è¯•")

    # 7. è·å–éœ€è¦çš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äºä¿®å¤ï¼‰
    from fix_build_issues import get_env_vars
    env_vars = get_env_vars(instance_id)

    # 7. åˆ›å»ºè¯„æµ‹ä»»åŠ¡
    print(f"\nğŸš€ åˆ›å»º SiFlow è¯„æµ‹ä»»åŠ¡...")

    timestamp = int(time.time())
    short_id = instance_id.split('__')[-1] if '__' in instance_id else instance_id[:10]
    short_ts = str(timestamp)[-6:]

    # æ ¹æ®patchç±»å‹è®¾ç½®ä¸åŒçš„å‰ç¼€
    if patch_type == "model":
        prefix_code = "mp"  # mp = model-patch
    else:
        prefix_code = "gf"  # gf = gold-fixed

    # æ„å»ºä»»åŠ¡åç§°ï¼šå¦‚æœæœ‰suffixåˆ™ä¸åŠ æ—¶é—´æˆ³ï¼Œå¦åˆ™åŠ æ—¶é—´æˆ³
    if task_name_suffix:
        task_name_prefix = f"eval-{short_id}-{prefix_code}-{task_name_suffix}"
    else:
        task_name_prefix = f"eval-{short_id}-{prefix_code}-{short_ts}"

    print(f"  âœ“ ä»»åŠ¡åç§°å‰ç¼€: {task_name_prefix} (patch_type: {patch_type})")

    # æ„å»ºtask_envåˆ—è¡¨
    task_env_list = [
        TaskEnv(env_key="INSTANCE_ID", env_value=instance_id, hide=False),
        TaskEnv(env_key="PATCH_TYPE", env_value=patch_type, hide=False),
        TaskEnv(env_key="EVAL_VERSION", env_value="fixed", hide=False),
    ]

    # æ·»åŠ ä¿®å¤æ‰€éœ€çš„ç¯å¢ƒå˜é‡
    if env_vars:
        for key, value in env_vars.items():
            task_env_list.append(TaskEnv(env_key=key, env_value=value, hide=False))
        print(f"  âœ“ æ·»åŠ ä¿®å¤ç¯å¢ƒå˜é‡: {list(env_vars.keys())}")

    try:
        task_uuid = client.tasks.create(
            name_prefix=task_name_prefix,
            image=instance_image_name,
            image_version=image_version,
            image_url=instance_image_url,
            image_type="custom",
            type="pytorchjob",
            priority="medium",
            cmd=eval_script,
            workers=0,
            resource_pool=RESOURCE_POOL,
            instances=[
                TaskUserSelectedInstance(name=INSTANCE_TYPE, count_per_pod=1)
            ],
            task_env=task_env_list,
            volumes=[
                TaskVolume(mount_dir="/volume/ai-infra", volume_id=1)
            ]
        )

        print(f"  âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ")
        print(f"     Task UUID: {task_uuid}")

    except Exception as e:
        print(f"\n  âŒ ä»»åŠ¡åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "instance_id": instance_id
        }

    # 8. å¦‚æœä¸ç­‰å¾…ï¼Œç›´æ¥è¿”å›
    if not wait:
        return {
            "success": True,
            "task_uuid": task_uuid,
            "instance_id": instance_id,
            "status": "submitted"
        }

    # 9. ç­‰å¾…ä»»åŠ¡å®Œæˆ
    print(f"\nâ³ ç­‰å¾…ä»»åŠ¡æ‰§è¡Œå®Œæˆ...")
    print()

    start_time = time.time()
    last_status = None
    check_interval = 30

    while time.time() - start_time < timeout:
        try:
            task = client.tasks.get(uuid=task_uuid)

            if task.status != last_status:
                elapsed = int(time.time() - start_time)
                elapsed_min = elapsed // 60
                elapsed_sec = elapsed % 60
                print(f"   [{elapsed_min:02d}:{elapsed_sec:02d}] çŠ¶æ€: {task.status}")
                last_status = task.status

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if task.status == "Succeeded":
                print()
                print(f"âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")

                # è¯»å–ä¿å­˜çš„æµ‹è¯•è¾“å‡º
                output_dir = Path("./eval_outputs")
                test_output_file = output_dir / f"{instance_id}_test_output.txt"

                # ç­‰å¾…æ–‡ä»¶å†™å…¥ï¼ˆæœ€å¤š10ç§’ï¼‰
                import time as time_module
                for _ in range(10):
                    if test_output_file.exists():
                        break
                    time_module.sleep(1)

                try:
                    print(f"\nğŸ“‹ è¯»å–æµ‹è¯•ç»“æœ...")

                    if test_output_file.exists():
                        test_output = test_output_file.read_text()
                        print(f"  âœ“ æµ‹è¯•è¾“å‡ºå·²è¯»å–: {test_output_file}")

                        # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•è¾“å‡ºæ ‡è®°
                        if START_TEST_OUTPUT not in test_output or END_TEST_OUTPUT not in test_output:
                            print(f"  âš ï¸  è­¦å‘Š: æµ‹è¯•è¾“å‡ºä¸­ç¼ºå°‘æ ‡è®°")
                        else:
                            print(f"  âœ“ æ‰¾åˆ°æµ‹è¯•è¾“å‡ºæ ‡è®°")

                        # ä»æµ‹è¯•è¾“å‡ºä¸­æå–exit_code
                        exit_code = -1
                        if "SWEBENCH_TEST_EXIT_CODE=" in test_output:
                            try:
                                for line in test_output.split('\n'):
                                    if 'SWEBENCH_TEST_EXIT_CODE=' in line:
                                        exit_code = int(line.split('=')[1].strip())
                                        break
                            except (ValueError, IndexError):
                                exit_code = -1

                        # ä½¿ç”¨log parserè§£ææµ‹è¯•ç»“æœ
                        repo = instance['repo']
                        log_parser = MAP_REPO_TO_PARSER[repo]
                        status_map = log_parser(test_output, instance)

                        print(f"  âœ“ è§£æåˆ° {len(status_map)} ä¸ªæµ‹è¯•ç»“æœ")

                        # è·å–FAIL_TO_PASSå’ŒPASS_TO_PASSæµ‹è¯•åˆ—è¡¨
                        import json
                        fail_to_pass_str = instance.get('FAIL_TO_PASS', '[]')
                        pass_to_pass_str = instance.get('PASS_TO_PASS', '[]')

                        fail_to_pass = json.loads(fail_to_pass_str) if isinstance(fail_to_pass_str, str) else fail_to_pass_str
                        pass_to_pass = json.loads(pass_to_pass_str) if isinstance(pass_to_pass_str, str) else pass_to_pass_str

                        gold_results = {
                            FAIL_TO_PASS: fail_to_pass,
                            PASS_TO_PASS: pass_to_pass
                        }

                        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
                        report = get_eval_tests_report(status_map, gold_results)
                        resolution_status = get_resolution_status(report)

                        # åˆ¤æ–­æ˜¯å¦resolved
                        resolved = (resolution_status == "RESOLVED_FULL")

                        # æ‰“å°è¯¦ç»†çš„æµ‹è¯•ç»“æœ
                        print(f"\n  ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
                        print(f"  {'='*60}")

                        # FAIL_TO_PASS
                        f2p_pass = len(report[FAIL_TO_PASS]['success'])
                        f2p_total = len(fail_to_pass)
                        print(f"\n  ğŸ¯ FAIL_TO_PASS: {f2p_pass}/{f2p_total} passed")

                        if f2p_pass > 0:
                            print(f"     âœ… æˆåŠŸ:")
                            for test in report[FAIL_TO_PASS]['success'][:3]:
                                print(f"        â€¢ {test}")
                            if f2p_pass > 3:
                                print(f"        ... åŠå…¶ä»– {f2p_pass - 3} ä¸ª")

                        if report[FAIL_TO_PASS]['failure']:
                            print(f"     âŒ å¤±è´¥:")
                            for test in report[FAIL_TO_PASS]['failure'][:3]:
                                print(f"        â€¢ {test}")
                            if len(report[FAIL_TO_PASS]['failure']) > 3:
                                print(f"        ... åŠå…¶ä»– {len(report[FAIL_TO_PASS]['failure']) - 3} ä¸ª")

                        # PASS_TO_PASS
                        p2p_pass = len(report[PASS_TO_PASS]['success'])
                        p2p_total = len(pass_to_pass)
                        print(f"\n  ğŸ›¡ï¸  PASS_TO_PASS: {p2p_pass}/{p2p_total} passed")

                        if report[PASS_TO_PASS]['failure']:
                            print(f"     âš ï¸  å›å½’:")
                            for test in report[PASS_TO_PASS]['failure'][:3]:
                                print(f"        â€¢ {test}")
                            if len(report[PASS_TO_PASS]['failure']) > 3:
                                print(f"        ... åŠå…¶ä»– {len(report[PASS_TO_PASS]['failure']) - 3} ä¸ª")

                        print(f"\n  {'='*60}")
                        print(f"  æœ€ç»ˆçŠ¶æ€: {resolution_status}")
                        print(f"  é€€å‡ºç : {exit_code}")

                        if resolved:
                            print(f"  âœ… RESOLVED_FULL - Gold Patch å®Œå…¨è§£å†³é—®é¢˜")
                        else:
                            print(f"  âŒ {resolution_status} - Gold Patch æœªå®Œå…¨è§£å†³é—®é¢˜")
                    else:
                        print(f"  âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•è¾“å‡ºæ–‡ä»¶: {test_output_file}")
                        resolved = False

                except Exception as e:
                    print(f"  âš ï¸  è¯»å–ç»“æœå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    resolved = False

                return {
                    "success": True,
                    "instance_id": instance_id,
                    "task_uuid": task_uuid,
                    "resolved": resolved,
                    "resolution_status": resolution_status if 'resolution_status' in locals() else "UNKNOWN",
                    "exit_code": exit_code if 'exit_code' in locals() else -1,
                    "report": report if 'report' in locals() else None,
                    "execution_time": int(time.time() - start_time),
                    "test_output_file": str(test_output_file) if test_output_file.exists() else None
                }

            elif task.status in ["Failed", "Error"]:
                print()
                print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.status}")
                return {
                    "success": False,
                    "instance_id": instance_id,
                    "task_uuid": task_uuid,
                    "status": "failed"
                }

        except Exception as e:
            print(f"   âš ï¸  æŸ¥è¯¢å¤±è´¥: {e}")

        time.sleep(check_interval)

    # è¶…æ—¶
    print()
    print(f"âŒ ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ (>{timeout}ç§’)")
    return {
        "success": False,
        "instance_id": instance_id,
        "task_uuid": task_uuid,
        "status": "timeout"
    }


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="è¿è¡Œ gold patch è¯„æµ‹ (ä¿®å¤ç‰ˆæœ¬)")
    parser.add_argument("instance_id", help="Instance ID")
    parser.add_argument("--version", default="2.0.0", help="é•œåƒç‰ˆæœ¬")
    parser.add_argument("--timeout", type=int, default=1800, help="è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--wait", action="store_true", help="ç­‰å¾…ä»»åŠ¡å®Œæˆ")

    args = parser.parse_args()

    result = run_gold_eval_for_instance(
        instance_id=args.instance_id,
        image_version=args.version,
        timeout=args.timeout,
        wait=args.wait
    )

    print("\n" + "="*70)
    if result.get("success"):
        if result.get("resolved"):
            print(f"ğŸ‰ è¯„æµ‹æˆåŠŸ - RESOLVED_FULL")
        elif result.get("status") == "submitted":
            print("âœ… ä»»åŠ¡å·²æäº¤")
            print(f"Task UUID: {result.get('task_uuid')}")
        else:
            print(f"âš ï¸  è¯„æµ‹å®Œæˆ - {result.get('resolution_status', 'UNKNOWN')}")
        if result.get("test_output_file"):
            print(f"æµ‹è¯•è¾“å‡º: {result.get('test_output_file')}")
        return 0 if result.get("resolved") else 1
    else:
        print("âŒ è¯„æµ‹å¤±è´¥")
        print(f"é”™è¯¯: {result.get('error', 'Unknown')}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
