>>>>> Start Test Output
+ export PYTHONPATH=/testbed:
+ PYTHONPATH=/testbed:
+ echo PYTHONPATH=/testbed:
PYTHONPATH=/testbed:
+ pytest -rA sklearn/linear_model/tests/test_huber.py
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.4, py-1.11.0, pluggy-0.13.1
rootdir: /testbed, configfile: setup.cfg
collected 10 items

sklearn/linear_model/tests/test_huber.py F..FFFFFFF                      [100%]

=================================== FAILURES ===================================
____________________ test_huber_equals_lr_for_high_epsilon _____________________

    def test_huber_equals_lr_for_high_epsilon():
        # Test that Ridge matches LinearRegression for large epsilon
        X, y = make_regression_with_outliers()
        lr = LinearRegression(fit_intercept=True)
        lr.fit(X, y)
        huber = HuberRegressor(fit_intercept=True, epsilon=1e3, alpha=0.0)
>       huber.fit(X, y)

sklearn/linear_model/tests/test_huber.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.0, epsilon=1000.0, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[ 1.22991463e+00,  2.85932153e+00, -4.23904513e-01,
        -1.60674512e-01,  8.10795568e-01,  2.37213170e-01,
...-1.37495129e+00,
        -1.71546331e-01, -1.10438334e+00, -1.60205766e+00,
         6.25231451e-01,  1.33652795e+00]])
y = array([ 1.20866536e+02,  1.65803518e+02, -1.53756503e+02,  4.69095117e+02,
       -2.86860094e+02,  5.35979427e+01,  1...2,
       -2.02361875e+02,  3.47374988e+01, -8.03250608e+01,  2.70115543e+02,
        1.18312506e+01, -5.23890666e+02])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
__________________________ test_huber_sample_weights ___________________________

    def test_huber_sample_weights():
        # Test sample_weights implementation in HuberRegressor"""
    
        X, y = make_regression_with_outliers()
        huber = HuberRegressor(fit_intercept=True)
>       huber.fit(X, y)

sklearn/linear_model/tests/test_huber.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[ 1.22991463e+00,  2.85932153e+00, -4.23904513e-01,
        -1.60674512e-01,  8.10795568e-01,  2.37213170e-01,
...-1.37495129e+00,
        -1.71546331e-01, -1.10438334e+00, -1.60205766e+00,
         6.25231451e-01,  1.33652795e+00]])
y = array([ 1.20866536e+02,  1.65803518e+02, -1.53756503e+02,  4.69095117e+02,
       -2.86860094e+02,  5.35979427e+01,  1...2,
       -2.02361875e+02,  3.47374988e+01, -8.03250608e+01,  2.70115543e+02,
        1.18312506e+01, -5.23890666e+02])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
______________________________ test_huber_sparse _______________________________

    def test_huber_sparse():
        X, y = make_regression_with_outliers()
        huber = HuberRegressor(fit_intercept=True, alpha=0.1)
>       huber.fit(X, y)

sklearn/linear_model/tests/test_huber.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.1, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[ 1.22991463e+00,  2.85932153e+00, -4.23904513e-01,
        -1.60674512e-01,  8.10795568e-01,  2.37213170e-01,
...-1.37495129e+00,
        -1.71546331e-01, -1.10438334e+00, -1.60205766e+00,
         6.25231451e-01,  1.33652795e+00]])
y = array([ 1.20866536e+02,  1.65803518e+02, -1.53756503e+02,  4.69095117e+02,
       -2.86860094e+02,  5.35979427e+01,  1...2,
       -2.02361875e+02,  3.47374988e+01, -8.03250608e+01,  2.70115543e+02,
        1.18312506e+01, -5.23890666e+02])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
_________________________ test_huber_scaling_invariant _________________________

    def test_huber_scaling_invariant():
        # Test that outliers filtering is scaling independent.
        X, y = make_regression_with_outliers()
        huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100)
>       huber.fit(X, y)

sklearn/linear_model/tests/test_huber.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.0, epsilon=1.35, fit_intercept=False, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[ 1.22991463e+00,  2.85932153e+00, -4.23904513e-01,
        -1.60674512e-01,  8.10795568e-01,  2.37213170e-01,
...-1.37495129e+00,
        -1.71546331e-01, -1.10438334e+00, -1.60205766e+00,
         6.25231451e-01,  1.33652795e+00]])
y = array([ 1.20866536e+02,  1.65803518e+02, -1.53756503e+02,  4.69095117e+02,
       -2.86860094e+02,  5.35979427e+01,  1...2,
       -2.02361875e+02,  3.47374988e+01, -8.03250608e+01,  2.70115543e+02,
        1.18312506e+01, -5.23890666e+02])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
_______________________ test_huber_and_sgd_same_results ________________________

    @pytest.mark.filterwarnings('ignore:max_iter and tol parameters have been')
    def test_huber_and_sgd_same_results():
        # Test they should converge to same coefficients for same parameters
    
        X, y = make_regression_with_outliers(n_samples=10, n_features=2)
    
        # Fit once to find out the scale parameter. Scale down X and y by scale
        # so that the scale parameter is optimized to 1.0
        huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100,
                               epsilon=1.35)
>       huber.fit(X, y)

sklearn/linear_model/tests/test_huber.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.0, epsilon=1.35, fit_intercept=False, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[ 1.76405235,  0.40015721],
       [ 0.3130677 , -0.85409574],
       [ 1.86755799, -0.97727788],
       [ 0.95... 0.33367433],
       [ 0.97873798,  2.2408932 ],
       [ 1.49407907, -0.20515826],
       [-0.10321885,  0.4105985 ]])
y = array([ 62.96222847, -76.12237335, -65.5051241 ,  -0.71552346,
       139.37289155,  22.40627451,  37.87474271, 225.75342685,
         2.11409015,  37.42177578])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
____________________________ test_huber_warm_start _____________________________

    def test_huber_warm_start():
        X, y = make_regression_with_outliers()
        huber_warm = HuberRegressor(
            fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True,
            tol=1e-1)
>       huber_warm.fit(X, y)

sklearn/linear_model/tests/test_huber.py:176: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=1.0, epsilon=1.35, fit_intercept=True, max_iter=10000,
               tol=0.1, warm_start=True)
X = array([[ 1.22991463e+00,  2.85932153e+00, -4.23904513e-01,
        -1.60674512e-01,  8.10795568e-01,  2.37213170e-01,
...-1.37495129e+00,
        -1.71546331e-01, -1.10438334e+00, -1.60205766e+00,
         6.25231451e-01,  1.33652795e+00]])
y = array([ 1.20866536e+02,  1.65803518e+02, -1.53756503e+02,  4.69095117e+02,
       -2.86860094e+02,  5.35979427e+01,  1...2,
       -2.02361875e+02,  3.47374988e+01, -8.03250608e+01,  2.70115543e+02,
        1.18312506e+01, -5.23890666e+02])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
__________________________ test_huber_better_r2_score __________________________

    def test_huber_better_r2_score():
        # Test that huber returns a better r2 score than non-outliers"""
        X, y = make_regression_with_outliers()
        huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)
>       huber.fit(X, y)

sklearn/linear_model/tests/test_huber.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.01, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[ 1.22991463e+00,  2.85932153e+00, -4.23904513e-01,
        -1.60674512e-01,  8.10795568e-01,  2.37213170e-01,
...-1.37495129e+00,
        -1.71546331e-01, -1.10438334e+00, -1.60205766e+00,
         6.25231451e-01,  1.33652795e+00]])
y = array([ 1.20866536e+02,  1.65803518e+02, -1.53756503e+02,  4.69095117e+02,
       -2.86860094e+02,  5.35979427e+01,  1...2,
       -2.02361875e+02,  3.47374988e+01, -8.03250608e+01,  2.70115543e+02,
        1.18312506e+01, -5.23890666e+02])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
_______________________________ test_huber_bool ________________________________

    def test_huber_bool():
        # Test that it does not crash with bool data
        X, y = make_regression(n_samples=200, n_features=2, noise=4.0,
                               random_state=0)
        X_bool = X > 0
>       HuberRegressor().fit(X_bool, y)

sklearn/linear_model/tests/test_huber.py:215: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
X = array([[False, False],
       [False,  True],
       [False,  True],
       [ True,  True],
       [False,  True],
   ...e],
       [False, False],
       [ True,  True],
       [ True, False],
       [False, False],
       [False,  True]])
y = array([-36.74055607,   7.19003131,  44.38901836,  57.01951456,
       -16.08855418,  -8.57916018, -31.39240867,  19.01...-87.29054611, -31.60922508,  37.50568861,  -2.62204748,
        56.94948602, -43.52307458, -31.06118393,  34.93490838])
sample_weight = array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., ...1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,)
            Weight given to each sample.
    
        Returns
        -------
        self : object
        """
        X, y = check_X_y(
            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
        if sample_weight is not None:
            sample_weight = np.array(sample_weight)
            check_consistent_length(y, sample_weight)
        else:
            sample_weight = np.ones_like(y)
    
        if self.epsilon < 1.0:
            raise ValueError(
                "epsilon should be greater than or equal to 1.0, got %f"
                % self.epsilon)
    
        if self.warm_start and hasattr(self, 'coef_'):
            parameters = np.concatenate(
                (self.coef_, [self.intercept_, self.scale_]))
        else:
            if self.fit_intercept:
                parameters = np.zeros(X.shape[1] + 2)
            else:
                parameters = np.zeros(X.shape[1] + 1)
            # Make sure to initialize the scale parameter to a strictly
            # positive value:
            parameters[-1] = 1
    
        # Sigma or the scale factor should be non-negative.
        # Setting it to be zero might cause undefined bounds hence we set it
        # to a value close to zero.
        bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))
        bounds[-1][0] = np.finfo(np.float64).eps * 10
    
        parameters, f, dict_ = optimize.fmin_l_bfgs_b(
            _huber_loss_and_gradient, parameters,
            args=(X, y, self.epsilon, self.alpha, sample_weight),
            maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,
            iprint=0)
        if dict_['warnflag'] == 2:
            raise ValueError("HuberRegressor convergence failed:"
                             " l-BFGS-b solver terminated with %s"
>                            % dict_['task'].decode('ascii'))
E           ValueError: HuberRegressor convergence failed: l-BFGS-b solver terminated with ABNORMAL_TERMINATION_IN_LNSRCH

sklearn/linear_model/huber.py:292: ValueError
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED sklearn/linear_model/tests/test_huber.py::test_huber_max_iter
PASSED sklearn/linear_model/tests/test_huber.py::test_huber_gradient
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_equals_lr_for_high_epsilon
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_sample_weights - ...
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_sparse - ValueErr...
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_scaling_invariant
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_and_sgd_same_results
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_warm_start - Valu...
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_better_r2_score
FAILED sklearn/linear_model/tests/test_huber.py::test_huber_bool - ValueError...
========================= 8 failed, 2 passed in 1.17s ==========================
>>>>> End Test Output
SWEBENCH_TEST_EXIT_CODE=1
